import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import optuna

# Load the Titanic dataset
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
data = pd.read_csv(url)

# Feature Engineering
data['Age'].fillna(data['Age'].median(), inplace=True)
data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)
data['Fare'].fillna(data['Fare'].median(), inplace=True)
data['FamilySize'] = data['SibSp'] + data['Parch'] + 1
data['IsAlone'] = (data['FamilySize'] == 1).astype(int)
data['FarePerPerson'] = data['Fare'] / data['FamilySize']

# Create interaction features
data['Age*Class'] = data['Age'] * data['Pclass']
data['Fare*Class'] = data['Fare'] * data['Pclass']

# Drop irrelevant columns
data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)

# Encode categorical variables
data = pd.get_dummies(data, columns=['Sex', 'Embarked'], drop_first=True)

# Feature selection and scaling
features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone',
            'FarePerPerson', 'Age*Class', 'Fare*Class', 'Sex_male', 'Embarked_Q', 'Embarked_S']
target = 'Survived'
X = data[features]
y = data[target]

scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=features)

# Split the dataset for hyperparameter tuning
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Hyperparameter Optimization with Optuna
def objective(trial):
    model_type = trial.suggest_categorical("model_type", ["RandomForest", "XGBoost", "GradientBoosting"])
    
    if model_type == "RandomForest":
        n_estimators = trial.suggest_int("n_estimators", 100, 300)
        max_depth = trial.suggest_int("max_depth", 5, 50)
        min_samples_split = trial.suggest_int("min_samples_split", 2, 15)
        min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 10)
        max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", None])
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            random_state=42
        )
    
    elif model_type == "XGBoost":
        learning_rate = trial.suggest_float("learning_rate", 0.01, 0.3)
        n_estimators = trial.suggest_int("n_estimators", 100, 300)
        max_depth = trial.suggest_int("max_depth", 3, 10)
        subsample = trial.suggest_float("subsample", 0.5, 1.0)
        colsample_bytree = trial.suggest_float("colsample_bytree", 0.5, 1.0)
        model = XGBClassifier(
            learning_rate=learning_rate,
            n_estimators=n_estimators,
            max_depth=max_depth,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            random_state=42,
            use_label_encoder=False,
            eval_metric="logloss"
        )
    
    else:  # GradientBoosting
        learning_rate = trial.suggest_float("learning_rate", 0.01, 0.3)
        n_estimators = trial.suggest_int("n_estimators", 100, 300)
        max_depth = trial.suggest_int("max_depth", 3, 10)
        model = GradientBoostingClassifier(
            learning_rate=learning_rate,
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42
        )
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    return accuracy_score(y_test, predictions)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=150)

# Extract the best parameters and retrain the model
optimized_params = study.best_params
print("Optimized Hyperparameters:", optimized_params)

# Remove 'model_type' key before initializing the model
if optimized_params["model_type"] == "RandomForest":
    rf_params = {k: v for k, v in optimized_params.items() if k != "model_type"}
    final_model = RandomForestClassifier(**rf_params, random_state=42)
elif optimized_params["model_type"] == "XGBoost":
    xgb_params = {k: v for k, v in optimized_params.items() if k != "model_type"}
    final_model = XGBClassifier(**xgb_params, random_state=42, use_label_encoder=False, eval_metric="logloss")
else:
    gb_params = {k: v for k, v in optimized_params.items() if k != "model_type"}
    final_model = GradientBoostingClassifier(**gb_params, random_state=42)

final_model.fit(X_train, y_train)

# Evaluate the final model
final_predictions = final_model.predict(X_test)
final_accuracy = accuracy_score(y_test, final_predictions)
print("Final Model Accuracy:", final_accuracy)
